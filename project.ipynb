{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import cohen_kappa_score, precision_score, recall_score, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "batch_size = 100\n",
    "num_classes = 5  # 5 DR levels\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "class DeepDRiD(Dataset):\n",
    "    def __init__(self, ann_file, image_dir, transform=None, mode='dual', test=False):\n",
    "        self.ann_file = ann_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.test = test\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == 'single':\n",
    "            self.data = self.load_data()\n",
    "        else:\n",
    "            self.data = self.load_data_dual()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'single':\n",
    "            return self.get_item(index)\n",
    "        else:\n",
    "            return self.get_item_dual(index)\n",
    "\n",
    "    # 1. single image\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_info = dict()\n",
    "            file_info['img_path'] = os.path.join(\n",
    "                self.image_dir, row['img_path'])\n",
    "            if not self.test:\n",
    "                file_info['dr_level'] = int(row['patient_DR_Level'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item(self, index):\n",
    "        data = self.data[index]\n",
    "        img = Image.open(data['img_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if not self.test:\n",
    "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "            return img, label\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    # 2. dual image\n",
    "    def load_data_dual(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        df['prefix'] = df['image_id'].str.split(\n",
    "            '_').str[0]  # The patient id of each image\n",
    "        df['suffix'] = df['image_id'].str.split(\n",
    "            '_').str[1].str[0]  # The left or right eye\n",
    "        grouped = df.groupby(['prefix', 'suffix'])\n",
    "\n",
    "        data = []\n",
    "        for (prefix, suffix), group in grouped:\n",
    "            file_info = dict()\n",
    "            file_info['img_path1'] = os.path.join(\n",
    "                self.image_dir, group.iloc[0]['img_path'])\n",
    "            file_info['img_path2'] = os.path.join(\n",
    "                self.image_dir, group.iloc[1]['img_path'])\n",
    "            if not self.test:\n",
    "                file_info['dr_level'] = int(group.iloc[0]['patient_DR_Level'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item_dual(self, index):\n",
    "        data = self.data[index]\n",
    "        img1 = Image.open(data['img_path1']).convert('RGB')\n",
    "        img2 = Image.open(data['img_path2']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        if not self.test:\n",
    "            label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "            return [img1, img2], label\n",
    "        else:\n",
    "            return [img1, img2]\n",
    "\n",
    "\n",
    "def create_weighted_sampler_for_dataset(dataset):\n",
    "    num_samples = len(dataset)\n",
    "    labels = [dataset.__getitem__(i)[1].item() for i in range(num_samples)]\n",
    "    \n",
    "    c = Counter(labels)\n",
    "    class_counts = list(dict(sorted(c.items(), key = lambda i: i[0])).values())\n",
    "    print(class_counts)\n",
    "    class_weights = [num_samples / class_counts[i] for i in range(len(class_counts))]\n",
    "    weights = [class_weights[labels[i]] for i in range(num_samples)]\n",
    "\n",
    "    return torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), max(class_counts) * len(class_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class APTOS2019(Dataset):\n",
    "    def __init__(self, ann_file, image_dir, transform=None, mode='dual'):\n",
    "        self.ann_file = ann_file\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'single':\n",
    "            return self.get_item(index)\n",
    "        else:\n",
    "            return self.get_item_dual(index)\n",
    "\n",
    "    def load_data(self):\n",
    "        df = pd.read_csv(self.ann_file)\n",
    "\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            file_info = dict()\n",
    "            if self.mode == 'single':\n",
    "                file_info['img_path'] = os.path.join(\n",
    "                    self.image_dir, row['id_code']) + '.png'\n",
    "            else:\n",
    "                file_info['img_path1'] = os.path.join(\n",
    "                    self.image_dir, row['id_code']) + '.png'\n",
    "                file_info['img_path2'] = file_info['img_path1']\n",
    "            file_info['dr_level'] = int(row['diagnosis'])\n",
    "            data.append(file_info)\n",
    "        return data\n",
    "\n",
    "    def get_item(self, index):\n",
    "        data = self.data[index]\n",
    "        img = Image.open(data['img_path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "        return img, label\n",
    "\n",
    "    def get_item_dual(self, index):\n",
    "        data = self.data[index]\n",
    "        img1 = Image.open(data['img_path1']).convert('RGB')\n",
    "        img2 = Image.open(data['img_path2']).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = torch.tensor(data['dr_level'], dtype=torch.int64)\n",
    "        return [img1, img2], label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutOut(object):\n",
    "    def __init__(self, mask_size, p=0.5):\n",
    "        self.mask_size = mask_size\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() > self.p:\n",
    "            return img\n",
    "\n",
    "        # Ensure the image is a tensor\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            raise TypeError('Input image must be a torch.Tensor')\n",
    "\n",
    "        # Get height and width of the image\n",
    "        h, w = img.shape[1], img.shape[2]\n",
    "        mask_size_half = self.mask_size // 2\n",
    "        offset = 1 if self.mask_size % 2 == 0 else 0\n",
    "\n",
    "        cx = np.random.randint(mask_size_half, w + offset - mask_size_half)\n",
    "        cy = np.random.randint(mask_size_half, h + offset - mask_size_half)\n",
    "\n",
    "        xmin, xmax = cx - mask_size_half, cx + mask_size_half + offset\n",
    "        ymin, ymax = cy - mask_size_half, cy + mask_size_half + offset\n",
    "        xmin, xmax = max(0, xmin), min(w, xmax)\n",
    "        ymin, ymax = max(0, ymin), min(h, ymax)\n",
    "\n",
    "        img[:, ymin:ymax, xmin:xmax] = 0\n",
    "        return img\n",
    "\n",
    "\n",
    "class SLORandomPad:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        pad_width = max(0, self.size[0] - img.width)\n",
    "        pad_height = max(0, self.size[1] - img.height)\n",
    "        pad_left = random.randint(0, pad_width)\n",
    "        pad_top = random.randint(0, pad_height)\n",
    "        pad_right = pad_width - pad_left\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        return transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom))\n",
    "\n",
    "\n",
    "class FundRandomRotate:\n",
    "    def __init__(self, prob, degree):\n",
    "        self.prob = prob\n",
    "        self.degree = degree\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.prob:\n",
    "            angle = random.uniform(-self.degree, self.degree)\n",
    "            return transforms.functional.rotate(img, angle)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomCrop((210, 210)),\n",
    "    #SLORandomPad((224, 224)),\n",
    "    FundRandomRotate(prob=0.5, degree=30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=(0.1, 0.9)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, criterion, optimizer, lr_scheduler, num_epochs=25,\n",
    "                checkpoint_path='model.pth'):\n",
    "    best_model = model.state_dict()\n",
    "    best_epoch = None\n",
    "    best_val_kappa = -1.0  # Initialize the best kappa score\n",
    "\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "        running_loss = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f'Training', unit=' batch', file=sys.stdout) as pbar:\n",
    "            for images, labels in train_loader:\n",
    "                if not isinstance(images, list):\n",
    "                    images = images.to(device)  # single image case\n",
    "                else:\n",
    "                    images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels.long())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    {'lr': f'{optimizer.param_groups[0][\"lr\"]:.1e}', 'Loss': f'{loss.item():.4f}'})\n",
    "                pbar.update(1)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss = sum(running_loss) / len(running_loss)\n",
    "\n",
    "        train_metrics = compute_metrics(all_preds, all_labels, per_class=True)\n",
    "        kappa, accuracy, precision, recall = train_metrics[:4]\n",
    "\n",
    "        print(f'[Train] Kappa: {kappa:.4f} Accuracy: {accuracy:.4f} '\n",
    "              f'Precision: {precision:.4f} Recall: {recall:.4f} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        if len(train_metrics) > 4:\n",
    "            precision_per_class, recall_per_class = train_metrics[4:]\n",
    "            for i, (precision, recall) in enumerate(zip(precision_per_class, recall_per_class)):\n",
    "                print(\n",
    "                    f'[Train] Class {i}: Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "        # Evaluation on the validation set at the end of each epoch\n",
    "        val_metrics, val_loss = evaluate_model(model, val_loader, device, criterion)\n",
    "        val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
    "        print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
    "              f'Precision: {val_precision:.4f} Recall: {val_recall:.4f} Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_kappa > best_val_kappa:\n",
    "            best_val_kappa = val_kappa\n",
    "            best_epoch = epoch\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, checkpoint_path)\n",
    "\n",
    "        print(f'[Val] Best kappa: {best_val_kappa:.4f}, Epoch {best_epoch}')\n",
    "\n",
    "        train_accuracies.append(accuracy)\n",
    "        train_losses.append(epoch_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    return model, train_accuracies, val_accuracies, train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion, test_only=False, prediction_path='./test_predictions.csv'):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_image_ids = []\n",
    "    all_losses = []\n",
    "\n",
    "    with tqdm(total=len(test_loader), desc=f'Evaluating', unit=' batch', file=sys.stdout) as pbar:\n",
    "        for i, data in enumerate(test_loader):\n",
    "\n",
    "            if test_only:\n",
    "                images = data\n",
    "            else:\n",
    "                images, labels = data\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                images = images.to(device)  # single image case\n",
    "            else:\n",
    "                images = [x.to(device) for x in images]  # dual images case\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(images)\n",
    "                if not test_only:\n",
    "                    all_losses.append(criterion(outputs.cpu(), labels.long()))\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            if not isinstance(images, list):\n",
    "                # single image case\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                image_ids = [\n",
    "                    os.path.basename(test_loader.dataset.data[idx]['img_path']) for idx in\n",
    "                    range(i * test_loader.batch_size, i *\n",
    "                          test_loader.batch_size + len(images))\n",
    "                ]\n",
    "                all_image_ids.extend(image_ids)\n",
    "                if not test_only:\n",
    "                    all_labels.extend(labels.numpy())\n",
    "            else:\n",
    "                # dual images case\n",
    "                for k in range(2):\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    image_ids = [\n",
    "                        os.path.basename(test_loader.dataset.data[idx][f'img_path{k + 1}']) for idx in\n",
    "                        range(i * test_loader.batch_size, i *\n",
    "                              test_loader.batch_size + len(images[k]))\n",
    "                    ]\n",
    "                    all_image_ids.extend(image_ids)\n",
    "                    if not test_only:\n",
    "                        all_labels.extend(labels.numpy())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Save predictions to csv file for Kaggle online evaluation\n",
    "    if test_only:\n",
    "        df = pd.DataFrame({\n",
    "            'ID': all_image_ids,\n",
    "            'TARGET': all_preds\n",
    "        })\n",
    "        df.to_csv(prediction_path, index=False)\n",
    "        print(f'[Test] Save predictions to {os.path.abspath(prediction_path)}')\n",
    "    else:\n",
    "        metrics = compute_metrics(all_preds, all_labels)\n",
    "        return metrics, sum(all_losses)/len(all_losses)\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, per_class=False):\n",
    "    kappa = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(\n",
    "        labels, preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calculate and print precision and recall for each class\n",
    "    if per_class:\n",
    "        precision_per_class = precision_score(\n",
    "            labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(\n",
    "            labels, preds, average=None, zero_division=0)\n",
    "        return kappa, accuracy, precision, recall, precision_per_class, recall_per_class\n",
    "\n",
    "    return kappa, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mk473\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mk473\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use GPU device is possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "backbone = models.resnet18(pretrained=True)\n",
    "\n",
    "backbone_output_dim = 512\n",
    "\n",
    "\n",
    "class MyDualModel(nn.Module):\n",
    "    def __init__(self, num_classes=5, dropout_rate=0.6, attention=False, custom_backbone = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_attention = attention\n",
    "\n",
    "        _backbone = backbone\n",
    "        _backbone.fc = nn.Identity()\n",
    "        \n",
    "        if custom_backbone is not None:\n",
    "            _backbone.load_state_dict(custom_backbone)\n",
    "\n",
    "        # Here the two backbones will have the same structure but unshared weights\n",
    "        self.backbone1 = copy.deepcopy(_backbone)\n",
    "        self.backbone2 = copy.deepcopy(_backbone)\n",
    "\n",
    "        fc_input_dim = backbone_output_dim*2\n",
    "\n",
    "        if self.use_attention == True:\n",
    "\n",
    "            embed_dim = 512\n",
    "\n",
    "            self.q_embd = nn.Linear(backbone_output_dim*2, embed_dim)\n",
    "            self.k_embd = nn.Linear(backbone_output_dim*2, embed_dim)\n",
    "            self.v_embd = nn.Linear(backbone_output_dim*2, embed_dim)\n",
    "\n",
    "            self.att = nn.MultiheadAttention(embed_dim, 2, 0.2, batch_first=True)\n",
    "            #self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "            fc_input_dim = embed_dim\n",
    "\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        image1, image2 = images\n",
    "\n",
    "        x1 = self.backbone1(image1)\n",
    "        x2 = self.backbone2(image2)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "\n",
    "        if self.use_attention == True:\n",
    "            q = self.q_embd(x)\n",
    "            k = self.k_embd(x)\n",
    "            v = self.v_embd(x)\n",
    "\n",
    "            x = self.att(q, k, v, need_weights=False)[0]\n",
    "            #x = self.norm(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def freeze(self, doFreeze):\n",
    "        for p in self.backbone1.parameters(True):\n",
    "            p.requires_grad = not doFreeze\n",
    "        for p in self.backbone2.parameters(True):\n",
    "            p.requires_grad = not doFreeze\n",
    "\n",
    "\n",
    "class StackingModel(nn.Module):\n",
    "    def __init__(self, ensemble_models: list[nn.Module], num_classes=5, dropout_rate=0.6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ensemble_models = copy.deepcopy(ensemble_models)\n",
    "\n",
    "        for model in self.ensemble_models:\n",
    "            for p in model.parameters(True):\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.n_models = len(ensemble_models)\n",
    "\n",
    "        self.meta_model = nn.Sequential(\n",
    "            nn.Linear(self.n_models * num_classes, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        meta_inputs = [model(images) for model in self.ensemble_models]\n",
    "        meta_inputs = torch.cat(meta_inputs, dim=1)\n",
    "        \n",
    "        x = self.meta_model(meta_inputs)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class VotingModel(nn.Module):\n",
    "    def __init__(self, ensemble_models: list[nn.Module]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ensemble_models = copy.deepcopy(ensemble_models)\n",
    "\n",
    "        for model in self.ensemble_models:\n",
    "            for p in model.parameters(True):\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.n_models = len(ensemble_models)\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        ensemble_outputs = [model(images) for model in self.ensemble_models]\n",
    "\n",
    "        ensemble_outputs = torch.stack(ensemble_outputs, dim=0)\n",
    "\n",
    "        votes = torch.argmax(ensemble_outputs, dim=2).T\n",
    "\n",
    "        most_voted = torch.zeros_like(ensemble_outputs[0])\n",
    "        \n",
    "        for i, sample_votes in enumerate(votes):\n",
    "            max_voted = Counter(sample_votes).most_common(1)[0][0].item()\n",
    "            most_voted[i, max_voted] = 1.0\n",
    "\n",
    "        return most_voted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Mode: dual\n",
      "600\n",
      "2930\n",
      "[180, 120, 120, 120, 60]\n",
      "[1434, 300, 808, 154, 234]\n"
     ]
    }
   ],
   "source": [
    "# Choose between 'single image' and 'dual images' pipeline\n",
    "# This will affect the model definition, dataset pipeline, training and evaluation\n",
    "\n",
    "#mode = 'single'  # forward single image to the model each time\n",
    "mode = 'dual'  # forward two images of the same eye to the model and fuse the features\n",
    "\n",
    "assert mode in ('dual')\n",
    "\n",
    "print('Pipeline Mode:', mode)\n",
    "\n",
    "# Create datasets\n",
    "drid_train_dataset = DeepDRiD(\n",
    "\t'./DeepDRiD/train.csv', \n",
    "\t'./DeepDRiD/train/', \n",
    "\ttransform_train, mode)\n",
    "drid_val_dataset = DeepDRiD(\n",
    "\t'./DeepDRiD/val.csv', \n",
    "\t'./DeepDRiD/val/', \n",
    "\ttransform_test, mode)\n",
    "drid_test_dataset = DeepDRiD(\n",
    "\t'./DeepDRiD/test.csv', \n",
    "\t'./DeepDRiD/test/', \n",
    "\ttransform_test, mode, test=True)\n",
    "\n",
    "aptos_train_dataset = APTOS2019(\n",
    "\t'./APTOS-2019/train_1.csv', \n",
    "\t'./APTOS-2019/train_images/train_images/', \n",
    "\ttransform_train, mode)\n",
    "aptos_val_dataset = APTOS2019(\n",
    "\t'./APTOS-2019/valid.csv', \n",
    "\t'./APTOS-2019/val_images/val_images/', \n",
    "\ttransform_test, mode)\n",
    "\n",
    "print(len(drid_train_dataset))\n",
    "print(len(aptos_train_dataset))\n",
    "\n",
    "drid_train_sampler = create_weighted_sampler_for_dataset(drid_train_dataset)\n",
    "\n",
    "# Create dataloaders\n",
    "drid_train_loader = DataLoader(\n",
    "\tdrid_train_dataset, batch_size=batch_size, sampler=drid_train_sampler)\n",
    "drid_val_loader = DataLoader(drid_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "drid_test_loader = DataLoader(\n",
    "\tdrid_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "aptos_train_sampler = create_weighted_sampler_for_dataset(aptos_train_dataset)\n",
    "aptos_train_loader = DataLoader(\n",
    "\taptos_train_dataset, batch_size=batch_size, sampler=aptos_train_sampler)\n",
    "aptos_val_loader = DataLoader(aptos_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the weighted CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def print_stats(train_acc, val_acc, train_losses, val_losses, grad_cam_img = None):\n",
    "    f = plt.figure(figsize=(20, 5))\n",
    "    f.add_subplot(1, 2, 1)\n",
    "    plt.title(\"Accuracy per epoch\")\n",
    "    plt.plot(np.arange(len(train_acc)), train_acc, color='blue', label='Training')\n",
    "    plt.plot(np.arange(len(val_acc)), val_acc, color='red', label='Validation')\n",
    "    plt.legend()\n",
    "    f.add_subplot(1, 2, 2)\n",
    "    plt.title(\"Loss per epoch\")\n",
    "    plt.plot(np.arange(len(train_losses)), train_losses, color='blue', label='Training')\n",
    "    plt.plot(np.arange(len(val_losses)), val_losses, color='red', label='Validation')\n",
    "    plt.legend()\n",
    "    f.show()\n",
    "\n",
    "    if grad_cam_img is None:\n",
    "        return\n",
    "    \n",
    "    f2 = plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grad_cam_img)\n",
    "    f2.show()\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Source:\n",
    "https://medium.com/@codetrade/grad-cam-in-pytorch-a-powerful-tool-for-visualize-explanations-from-deep-networks-bdc7caf0b282\n",
    "\"\"\"\n",
    "def grad_cam(model, image_path):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = transform_test(img).unsqueeze(0)\n",
    "    img_tensor = img_tensor.cpu()\n",
    "\n",
    "    layer = model.backbone1.layer4\n",
    "    activations = []\n",
    "    gradients = []\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "    \n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "\n",
    "    layer.register_forward_hook(forward_hook)\n",
    "    layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    output = model([img_tensor, img_tensor])\n",
    "    pred = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[:, pred].backward()\n",
    "    \n",
    "    weights = torch.mean(gradients[0], dim=[0, 2, 3])\n",
    "    for i in range(activations[0].size()[1]):\n",
    "        activations[0][:, i, :, :] *= weights[i]\n",
    "    heatmap = torch.mean(activations[0], dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap.cpu().detach().numpy(), 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    heatmap = cv2.resize(heatmap, (img.size[1], img.size[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "    return Image.blend(img, to_pil_image(heatmap, mode='RGB'), 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_a():\n",
    "\n",
    "    model = MyDualModel()\n",
    "\n",
    "    #print(model, '\\n')\n",
    "    \n",
    "    # Move class weights to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer and Learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.25)\n",
    "\n",
    "    # Train and evaluate the model with the training and validation set\n",
    "    model, train_acc, val_acc, train_losses, val_losses = train_model(\n",
    "        model, drid_train_loader, drid_val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "        checkpoint_path='./model_a.pth'\n",
    "    )\n",
    "\n",
    "    # Load the pretrained checkpoint\n",
    "    state_dict = torch.load('./model_a.pth', map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Make predictions on testing set and save the prediction results\n",
    "    evaluate_model(model, drid_test_loader, device, criterion, test_only=True, prediction_path='./test_predictions_a.csv')\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    img_path = drid_train_dataset.data[0]['img_path1']\n",
    "    grad_cam_img = grad_cam(model, img_path)\n",
    "\n",
    "    print_stats(train_acc, val_acc, train_losses, val_losses, grad_cam_img)\n",
    "\n",
    "#task_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_b():\n",
    "    \n",
    "    aptos_finetune = False\n",
    "\n",
    "    if aptos_finetune:\n",
    "    \n",
    "        model = MyDualModel()\n",
    "\n",
    "        # Move class weights to the device\n",
    "        model = model.to(device)\n",
    "\n",
    "        model.freeze(False)\n",
    "\n",
    "        # Optimizer and Learning rate scheduler\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        # Train and evaluate the model with the training and validation set\n",
    "        model, train_acc, val_acc, train_losses, val_losses = train_model(\n",
    "            model, aptos_train_loader, aptos_val_loader, device, criterion, optimizer,\n",
    "            lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "            checkpoint_path='./model_b_aptos.pth'\n",
    "        )\n",
    "        \n",
    "        print_stats(train_acc, val_acc, train_losses, val_losses)\n",
    "\n",
    "        # Load the pretrained checkpoint\n",
    "        state_dict = torch.load('./model_b_aptos.pth', map_location=device)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        torch.save(model.backbone1.state_dict(), './ResNet18_aptos_pretrained.pth')\n",
    "\n",
    "    backbone = torch.load('./ResNet18_aptos_pretrained.pth')\n",
    "\n",
    "    model = MyDualModel(custom_backbone=backbone)\n",
    "    \n",
    "    # We leave all layers unfreezed for DeepDRiD fine-tuning too\n",
    "    #model.freeze(True)\n",
    "    model.freeze(False)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer and Learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Train and evaluate the model with the training and validation set\n",
    "    model, train_acc, val_acc, train_losses, val_losses = train_model(\n",
    "        model, drid_train_loader, drid_val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "        checkpoint_path='./model_b.pth'\n",
    "    )\n",
    "\n",
    "    # Load the pretrained checkpoint\n",
    "    state_dict = torch.load('./model_b.pth', map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Make predictions on testing set and save the prediction results\n",
    "    evaluate_model(model, drid_test_loader, device, criterion, test_only=True, prediction_path='./test_predictions_b.csv')\n",
    "    \n",
    "    model = model.to('cpu')\n",
    "    img_path = drid_train_dataset.data[0]['img_path1']\n",
    "    grad_cam_img = grad_cam(model, img_path)\n",
    "\n",
    "    print_stats(train_acc, val_acc, train_losses, val_losses, grad_cam_img)\n",
    "\n",
    "#task_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_c():\n",
    "\n",
    "    model = MyDualModel(attention = True)\n",
    "\n",
    "    #print(model, '\\n')\n",
    "    \n",
    "    # Move class weights to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer and Learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Train and evaluate the model with the training and validation set\n",
    "    model, train_acc, val_acc, train_losses, val_losses = train_model(\n",
    "        model, drid_train_loader, drid_val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=50,\n",
    "        checkpoint_path='./model_c.pth'\n",
    "    )\n",
    "\n",
    "    # Load the pretrained checkpoint\n",
    "    state_dict = torch.load('./model_c.pth', map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Make predictions on testing set and save the prediction results\n",
    "    evaluate_model(model, drid_test_loader, device, criterion, test_only=True, prediction_path='./test_predictions_c.csv')\n",
    "\n",
    "    model = model.to('cpu')\n",
    "    img_path = drid_train_dataset.data[0]['img_path1']\n",
    "    grad_cam_img = grad_cam(model, img_path)\n",
    "\n",
    "    print_stats(train_acc, val_acc, train_losses, val_losses, grad_cam_img)\n",
    "\n",
    "#task_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:02<00:00,  1.18s/ batch]\n",
      "[Val] Kappa: 0.6658 Accuracy: 0.5400 Precision: 0.5628 Recall: 0.5400 Loss: 1.3648\n",
      "Evaluating: 100%|██████████| 2/2 [00:02<00:00,  1.04s/ batch]\n",
      "[Test] Save predictions to c:\\Users\\mk473\\source\\repos\\deeplearning\\project\\test_predictions_d_voting.csv\n"
     ]
    }
   ],
   "source": [
    "model_paths = ['./model_b1.pth',\n",
    "                './model_b2.pth',\n",
    "                './model_b3.pth']\n",
    "\n",
    "models = [MyDualModel() for i in range(len(model_paths))]\n",
    "\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    models[i].load_state_dict(torch.load(model_path, map_location='cpu'), strict=True)\n",
    "\n",
    "def task_d_stacking():\n",
    "\n",
    "    model = StackingModel(models)\n",
    "\n",
    "    #print(model, '\\n')\n",
    "    \n",
    "    # Move class weights to the device\n",
    "    model = model.to(device)\n",
    "    for _model in model.ensemble_models:\n",
    "        _model.to(device)\n",
    "\n",
    "    # Optimizer and Learning rate scheduler\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    # Train and evaluate the model with the training and validation set\n",
    "    model, train_acc, val_acc, train_losses, val_losses = train_model(\n",
    "        model, drid_train_loader, drid_val_loader, device, criterion, optimizer,\n",
    "        lr_scheduler=lr_scheduler, num_epochs=num_epochs,\n",
    "        checkpoint_path='./model_d.pth'\n",
    "    )\n",
    "\n",
    "    # Load the pretrained checkpoint\n",
    "    state_dict = torch.load('./model_d.pth', map_location='cpu')\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # Make predictions on testing set and save the prediction results\n",
    "    evaluate_model(model, drid_test_loader, device, criterion, test_only=True, prediction_path='./test_predictions_d_stacking.csv')\n",
    "\n",
    "    print_stats(train_acc, val_acc, train_losses, val_losses)\n",
    "\n",
    "    \n",
    "def task_d_voting():\n",
    "    model = VotingModel(models)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for _model in model.ensemble_models:\n",
    "        _model.to(device)\n",
    "    val_metrics, val_loss = evaluate_model(model, drid_val_loader, device, criterion)\n",
    "    val_kappa, val_accuracy, val_precision, val_recall = val_metrics[:4]\n",
    "    print(f'[Val] Kappa: {val_kappa:.4f} Accuracy: {val_accuracy:.4f} '\n",
    "            f'Precision: {val_precision:.4f} Recall: {val_recall:.4f} Loss: {val_loss:.4f}')\n",
    "    evaluate_model(model, drid_test_loader, device, criterion, test_only=True, prediction_path='./test_predictions_d_voting.csv')\n",
    "\n",
    "\n",
    "#task_d_stacking()\n",
    "task_d_voting()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
